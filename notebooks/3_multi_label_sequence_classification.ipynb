{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bmarquescost\\anaconda3\\envs\\sa-data\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import transformers\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH  = 512\n",
    "TRAIN_RATIO = 0.85\n",
    "VAL_RATIO   = 0.15\n",
    "\n",
    "BATCH_SIZE  = 16 \n",
    "LEARNING_RATE = 2e-5\n",
    "EPOCHS = 5\n",
    "N_LABELS = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # Some cudnn methods can be random even after fixing the seed \n",
    "    # unless you tell it to be deterministic\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: go_emotions/simplified\n",
      "Found cached dataset go_emotions (C:/Users/bmarquescost/.cache/huggingface/datasets/go_emotions/simplified/0.0.0/2637cfdd4e64d30249c3ed2150fa2b9d279766bfcd6a809b9f085c61a90d776d)\n",
      "100%|██████████| 3/3 [00:00<00:00, 69.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43410, 3) (5426, 3) (5427, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My favourite food is anything I didn't have to...</td>\n",
       "      <td>[27]</td>\n",
       "      <td>eebbqej</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Now if he does off himself, everyone will thin...</td>\n",
       "      <td>[27]</td>\n",
       "      <td>ed00q6i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WHY THE FUCK IS BAYLESS ISOING</td>\n",
       "      <td>[2]</td>\n",
       "      <td>eezlygj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To make her feel threatened</td>\n",
       "      <td>[14]</td>\n",
       "      <td>ed7ypvh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dirty Southern Wankers</td>\n",
       "      <td>[3]</td>\n",
       "      <td>ed0bdzj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text labels       id\n",
       "0  My favourite food is anything I didn't have to...   [27]  eebbqej\n",
       "1  Now if he does off himself, everyone will thin...   [27]  ed00q6i\n",
       "2                     WHY THE FUCK IS BAYLESS ISOING    [2]  eezlygj\n",
       "3                        To make her feel threatened   [14]  ed7ypvh\n",
       "4                             Dirty Southern Wankers    [3]  ed0bdzj"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "go_emotions = load_dataset(\"go_emotions\")\n",
    "data = go_emotions.data\n",
    "\n",
    "train = data[\"train\"].to_pandas()\n",
    "valid = data[\"validation\"].to_pandas()\n",
    "test = data[\"test\"].to_pandas()\n",
    "\n",
    "print(train.shape, valid.shape, test.shape)\n",
    "# (43410, 3) (5426, 3) (5427, 3)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43410/43410 [00:01<00:00, 26812.84it/s]\n",
      "100%|██████████| 5426/5426 [00:00<00:00, 26861.46it/s]\n",
      "100%|██████████| 5427/5427 [00:00<00:00, 28119.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43410, 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encoder(df):\n",
    "    one_hot_encoding = []\n",
    "    for i in tqdm(range(len(df))):\n",
    "        temp = [0] * N_LABELS\n",
    "        label_indices = df.iloc[i][\"labels\"]\n",
    "        for index in label_indices:\n",
    "            temp[index] = 1\n",
    "        one_hot_encoding.append(temp)\n",
    "        \n",
    "    return pd.DataFrame(one_hot_encoding)\n",
    "\n",
    "train_ohe_labels = one_hot_encoder(train)\n",
    "valid_ohe_labels = one_hot_encoder(valid)\n",
    "test_ohe_labels = one_hot_encoder(test)\n",
    "\n",
    "print(train_ohe_labels.shape)\n",
    "#(43410, 28)\n",
    "\n",
    "train = pd.concat([train, train_ohe_labels], axis=1)\n",
    "valid = pd.concat([valid, valid_ohe_labels], axis=1)\n",
    "test = pd.concat([test, test_ohe_labels], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        self.len = len(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()} \n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(train, valid, tokenizer):\n",
    "    train_tokenized = tokenizer(train['text'].to_list(), return_tensors='pt', padding=True, truncation=True, max_length=MAX_LENGTH)\n",
    "    valid_tokenized = tokenizer(valid['text'].to_list(), return_tensors='pt', padding=True, truncation=True, max_length=MAX_LENGTH)\n",
    "\n",
    "    train_dataset = TextDataset(train_tokenized, train[range(N_LABELS)].values.tolist())\n",
    "    valid_dataset = TextDataset(valid_tokenized, valid[range(N_LABELS)].values.tolist())\n",
    "    \n",
    "    return train_dataset, valid_dataset\n",
    "\n",
    "def build_dataloader(train_dataset, valid_dataset):\n",
    "    train_data_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    valid_data_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    return train_data_loader, valid_data_loader\n",
    "\n",
    "def get_model(base):\n",
    "  model = AutoModelForSequenceClassification.from_pretrained(base, num_labels=N_LABELS)\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model):\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\"]\n",
    "    optimizer_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.001,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    opt = AdamW(optimizer_parameters, lr=LEARNING_RATE)\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scheduler(optimizer, num_train_steps):\n",
    "    sch = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=0, num_training_steps=num_train_steps)\n",
    "    return sch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logits, labels):\n",
    "    if labels is None:\n",
    "        return None\n",
    "    \n",
    "    loss_function = nn.BCEWithLogitsLoss()\n",
    "    return loss_function(logits.view(-1, len(labels)), labels.type_as(logits).view(-1, len(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics(preds, labels):\n",
    "    preds = torch.stack(preds)\n",
    "    preds = preds.cpu().detach().numpy()\n",
    "    labels = torch.stack(labels)\n",
    "    labels = labels.cpu().detach().numpy()\n",
    "    \n",
    "    fpr_micro, tpr_micro, _ = metrics.roc_curve(labels.ravel(), preds.ravel())\n",
    "    auc_micro = metrics.auc(fpr_micro, tpr_micro)\n",
    "    \n",
    "    return {\"auc_micro\": auc_micro}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(data_loader, model, optimizer, device, scheduler):\n",
    "    running_train_loss = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    bar = tqdm(total=len(data_loader), desc=f'Training', unit=\"steps\", position=0, leave=False)\n",
    "\n",
    "    for batch_id, batch in enumerate(data_loader):\n",
    "        ids, mask, labels = batch['input_ids'], batch['attention_mask'], batch['labels']\n",
    "        \n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        labels = labels.to(device, dtype=torch.float)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=ids, attention_mask=mask)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        running_train_loss += loss.item()\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        bar.update(1)\n",
    "\n",
    "    return running_train_loss\n",
    "    \n",
    "\n",
    "def eval_fn(data_loader, model, device):\n",
    "    eval_loss = 0.0\n",
    "    model.eval()\n",
    "    \n",
    "    fin_labels = []\n",
    "    fin_outputs = []\n",
    "   \n",
    "    bar = tqdm(total=len(data_loader), desc=f'Validation', unit=\"steps\", position=0, leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_id, batch in enumerate(data_loader):\n",
    "            ids, mask, labels = batch['input_ids'], batch['attention_mask'], batch['labels']\n",
    "\n",
    "            ids = ids.to(device, dtype=torch.long)\n",
    "            mask = mask.to(device, dtype=torch.long)\n",
    "            labels = labels.to(device, dtype=torch.float)\n",
    "\n",
    "            outputs = model(input_ids=ids, attention_mask=mask)\n",
    "\n",
    "            logits = outputs.logits\n",
    "\n",
    "            loss = loss_fn(logits, labels)\n",
    "            eval_loss += loss.item()\n",
    "            \n",
    "            fin_labels.extend(labels)\n",
    "            fin_outputs.extend(torch.sigmoid(logits))\n",
    "\n",
    "            bar.update(1)\n",
    "\n",
    "    return eval_loss, fin_outputs, fin_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer():\n",
    "    base = 'distilbert-base-uncased'\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(base)\n",
    "\n",
    "    train_dataset, valid_dataset = build_dataset(tokenizer)\n",
    "    train_data_loader, valid_data_loader = build_dataloader(train_dataset, valid_dataset)\n",
    "    \n",
    "    print(\"Length of Train Dataloader: \", len(train_data_loader))\n",
    "    print(\"Length of Valid Dataloader: \", len(valid_data_loader))\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    n_train_steps = int(len(train_dataset) / BATCH_SIZE * 10)\n",
    "\n",
    "    model = get_model(base)\n",
    "    optimizer = get_optimizer(model)\n",
    "    scheduler = get_scheduler(optimizer, n_train_steps)\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    log = {}\n",
    "\n",
    "    best_val_loss = 100\n",
    "    print('Training model')\n",
    "    \n",
    "    for epoch in tqdm(range(EPOCHS)):\n",
    "        train_loss = train_fn(train_data_loader, model, optimizer, device, scheduler)\n",
    "        eval_loss, preds, labels = eval_fn(valid_data_loader, model, device)\n",
    "        \n",
    "        auc_score = log_metrics(preds, labels)[\"auc_micro\"]\n",
    "        print(\"AUC score: \", auc_score)\n",
    "        avg_train_loss, avg_val_loss = train_loss / len(train_data_loader), eval_loss / len(valid_data_loader)\n",
    "        \n",
    "        log[epoch + 1] = {\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"val_loss\": avg_val_loss,\n",
    "            \"auc_score\": auc_score,\n",
    "        }\n",
    "\n",
    "        print(\"Average Train loss: \", avg_train_loss)\n",
    "        print(\"Average Valid loss: \", avg_val_loss)\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), \"./best_model.pt\")  \n",
    "            print(\"Model saved as current val_loss is: \", best_val_loss)  \n",
    "    \n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Train Dataloader:  2714\n",
      "Length of Valid Dataloader:  340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\bmarquescost\\anaconda3\\envs\\sa-data\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score:  0.9490670460823749\n",
      "Average Train loss:  0.12044907149748277\n",
      "Average Valid loss:  0.08805634068215594\n",
      "Model saved as current val_loss is:  0.08805634068215594\n",
      "Epoch 1 starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score:  0.9575188830185227\n",
      "Average Train loss:  0.08296550197393668\n",
      "Average Valid loss:  0.08330132675302379\n",
      "Model saved as current val_loss is:  0.08330132675302379\n",
      "Epoch 2 starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score:  0.9579044720232768\n",
      "Average Train loss:  0.07227193667809359\n",
      "Average Valid loss:  0.0837940646609401\n",
      "Epoch 3 starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score:  0.9554431703277195\n",
      "Average Train loss:  0.06205002740217687\n",
      "Average Valid loss:  0.08805994073695997\n",
      "Epoch 4 starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score:  0.9511291737515543\n",
      "Average Train loss:  0.05237170420662557\n",
      "Average Valid loss:  0.09458804812063189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "base = 'distilbert-base-uncased'\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(base)\n",
    "\n",
    "train_dataset, valid_dataset = build_dataset(train, valid, tokenizer)\n",
    "train_data_loader, valid_data_loader = build_dataloader(train_dataset, valid_dataset)\n",
    "\n",
    "print(\"Length of Train Dataloader: \", len(train_data_loader))\n",
    "print(\"Length of Valid Dataloader: \", len(valid_data_loader))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "n_train_steps = int(len(train_dataset) / BATCH_SIZE * 10)\n",
    "\n",
    "model = get_model(base)\n",
    "optimizer = get_optimizer(model)\n",
    "scheduler = get_scheduler(optimizer, n_train_steps)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "log = {}\n",
    "\n",
    "best_val_loss = np.inf\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch} starting')\n",
    "    train_loss = train_fn(train_data_loader, model, optimizer, device, scheduler)\n",
    "    eval_loss, preds, labels = eval_fn(valid_data_loader, model, device)\n",
    "    \n",
    "    auc_score = log_metrics(preds, labels)[\"auc_micro\"]\n",
    "    print(\"AUC score: \", auc_score)\n",
    "    avg_train_loss, avg_val_loss = train_loss / len(train_data_loader), eval_loss / len(valid_data_loader)\n",
    "    \n",
    "    log[epoch + 1] = {\n",
    "        \"train_loss\": avg_train_loss,\n",
    "        \"val_loss\": avg_val_loss,\n",
    "        \"auc_score\": auc_score,\n",
    "    }\n",
    "\n",
    "    print(\"Average Train loss: \", avg_train_loss)\n",
    "    print(\"Average Valid loss: \", avg_val_loss)\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), \"./best_model.pt\")  \n",
    "        print(\"Model saved as current val_loss is: \", best_val_loss) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sa-data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
